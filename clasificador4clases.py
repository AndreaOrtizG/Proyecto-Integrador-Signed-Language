# -*- coding: utf-8 -*-
"""Clasificador4clases.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXqjBayAM1UckEju2RZzIpkzSv5J8XO-
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import matplotlib.image as mpimg
import numpy as np
import math
dataTrain = "/content/drive/My Drive/SIGNED-LANGUAGE/imagenescuatro"
dataValidation= "/content/drive/My Drive/SIGNED-LANGUAGE/imagenescuatro"

directory_train = os.path.join(dataTrain, 'Train')
directory_validation = os.path.join(dataValidation, 'Validation')



def get_data(list_data, directory):
    arr = []
    for caracter_ima in list_data :
      nombre_caracter = caracter_ima
      #print(nombre_caracter)
      arr.append(mpimg.imread(directory_train+"/"+nombre_caracter))

    arr=np.array(arr)
    print(arr.shape)
    return arr

list_train = os.listdir(directory_train) # dir is your directory path
number_files = len(list_train)
train_arr= get_data(list_train, directory_train)
train_arr= train_arr.reshape(1138,64,64,1) #Especificar nuevo tamaño de la matriz, el 1 significa volverle a especificar que las imagenes son de un solo canal.

print(train_arr)

Letters=[ 0, 1 , 2 ,3]
size=1138/len(Letters)
labels_t=[]
for i in range(1138):
  aux=math.floor(i/size)
  labels_t.append(Letters[aux])

labels_t= np.array(labels_t)
labels_t=labels_t.reshape(1138,1)

labels_t.shape
print(labels_t)

def get_dataV(list_data, directory):
    arr = []
    for caracter_ima in list_data :
      nombre_caracter = caracter_ima
      #print(nombre_caracter)
      arr.append(mpimg.imread(directory_validation+"/"+nombre_caracter))

    arr=np.array(arr)
    print(arr.shape)
    return arr

list_v = os.listdir(directory_validation) # dir is your directory path
number_filesV = len(list_v)
valid_arr= get_dataV(list_v, directory_validation)
valid_arr= valid_arr.reshape(400,64,64,1)



LettersV=[ 0, 1 , 2 ,3]
sizeV=400/len(LettersV)
labels_v=[]
for i in range(400):
  aux=math.floor(i/sizeV)
  labels_v.append(LettersV[aux])

labels_v= np.array(labels_v)
labels_v=labels_v.reshape(400,1)

labels_v.shape
print(labels_v)

import sys
from tensorflow.python.keras import optimizers
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization
from tensorflow.python.keras.layers import Convolution2D, MaxPooling2D

epochs= 20      #Número de veces a iterar sobre todo el set de datos en el entrenamiento 
batch_size= 100        #Número de imagénes a procesar en cada uno de los pasos  
steps= 300       #Número de veces que se va a procesar la información por cada época
steps_validation= 100
filtersConv1= 16
filtersConv2= 32 
filtersConv3= 64
filtersConv4=128
size_filter1= (5,5)
size_filter2= (5,5)
size_filter3= (5,5)
classes= 2
lr= 0.005              #Learning rate

import tensorflow as tf

from tensorflow import keras

cnn= Sequential()

cnn.add(Convolution2D(filtersConv3,size_filter1, activation= 'relu', input_shape=(64,64,1)))  #Primera capa de convolución
##cnn.add(MaxPooling2D(2,2))
##cnn.add(BatchNormalization())
#cnn.add(Dropout(0.25))
cnn.add(Convolution2D(filtersConv2,size_filter2,activation='relu'))        #Segunda capa de convolución
#cnn.add(MaxPooling2D(2,2))
#cnn.add(BatchNormalization())
#cnn.add(Dropout(0.25))
#cnn.add(BatchNormalization())
cnn.add(Convolution2D(filtersConv3,size_filter3,activation= 'relu'))       #Tercera capa de convolución
##cnn.add(MaxPooling2D(2,2))
#cnn.add(Convolution2D(filtersConv4,(2,2),activation= 'relu'))       #Tercera capa de convolución
#cnn.add(MaxPooling2D(2,2))



cnn.add(Flatten())               #Aplanar la imagen
##cnn.add(Dense(128, activation= 'relu'))

cnn.add(Dense(64, activation='relu'))   #Primera capa fully conected
#cnn.add(Dropout(0.5))    

#cnn.add(BatchNormalization())               # Solo activa la mitad de las neuronas para evitar el sobreajuste

cnn.add(Dense(32, activation='relu'))  #Segunda capa fully conected
#cnn.add(Dropout(0.5))  

##cnn.add(BatchNormalization())                 # Solo activa la mitad de las neuronas para evitar el sobreajuste

cnn.add(Dense(4, activation='softmax'))     #Tercera capa fully conected con funcion de activación softmax

cnn.compile(loss= 'sparse_categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(learning_rate=0.001)
, metrics=['accuracy'])

cnn.summary()

cnn.fit(train_arr, labels_t , epochs=epochs, batch_size=batch_size)

test_loss, test_acc= cnn.evaluate(valid_arr,labels_v)
print(test_acc)